{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "runtime_attributes": {
        "runtime_version": "2025.07"
      },
      "authorship_tag": "ABX9TyPkNoU76FYPh+LGhnWl0cFz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jfahadi/Android-Project/blob/master/ECG_Differential.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this in your current Colab with Python 3.11.13\n",
        "!pip install --upgrade pip\n",
        "\n",
        "# Install all required packages\n",
        "!pip install tensorflow\n",
        "!pip install tensorflow-federated\n",
        "!pip install wfdb==4.1.0\n",
        "!pip install pandas\n",
        "!pip install scikit-learn\n",
        "!pip install imbalanced-learn\n",
        "!pip install matplotlib\n",
        "!pip install seaborn\n",
        "\n",
        "print(\"✅ All packages installed successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "XC5azObFEKi1",
        "outputId": "dc38b9d3-567a-4701-8de3-a4d5fd02a15a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (24.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-25.3-py3-none-any.whl.metadata (4.7 kB)\n",
            "Downloading pip-25.3-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 24.1.2\n",
            "    Uninstalling pip-24.1.2:\n",
            "      Successfully uninstalled pip-24.1.2\n",
            "Successfully installed pip-25.3\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.14.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.73.1)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.7.14)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Collecting tensorflow-federated\n",
            "  Downloading tensorflow_federated-0.87.0-py3-none-manylinux_2_31_x86_64.whl.metadata (19 kB)\n",
            "Requirement already satisfied: absl-py==1.*,>=1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow-federated) (1.4.0)\n",
            "Collecting attrs~=23.1 (from tensorflow-federated)\n",
            "  Downloading attrs-23.2.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: cachetools~=5.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow-federated) (5.5.2)\n",
            "Collecting dm-tree==0.1.8 (from tensorflow-federated)\n",
            "  Downloading dm_tree-0.1.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.9 kB)\n",
            "Collecting dp-accounting==0.4.3 (from tensorflow-federated)\n",
            "  Downloading dp_accounting-0.4.3-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting google-vizier==0.1.11 (from tensorflow-federated)\n",
            "  Downloading google_vizier-0.1.11-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: grpcio~=1.46 in /usr/local/lib/python3.11/dist-packages (from tensorflow-federated) (1.73.1)\n",
            "Collecting jaxlib==0.4.14 (from tensorflow-federated)\n",
            "  Downloading jaxlib-0.4.14-cp311-cp311-manylinux2014_x86_64.whl.metadata (2.0 kB)\n",
            "Collecting jax==0.4.14 (from tensorflow-federated)\n",
            "  Downloading jax-0.4.14.tar.gz (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ml-dtypes==0.2.*,>=0.2.0 (from tensorflow-federated)\n",
            "  Downloading ml_dtypes-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Collecting numpy~=1.25 (from tensorflow-federated)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Collecting portpicker~=1.6 (from tensorflow-federated)\n",
            "  Downloading portpicker-1.6.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting scipy~=1.9.3 (from tensorflow-federated)\n",
            "  Downloading scipy-1.9.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n",
            "Collecting tensorflow-model-optimization==0.7.5 (from tensorflow-federated)\n",
            "  Downloading tensorflow_model_optimization-0.7.5-py2.py3-none-any.whl.metadata (914 bytes)\n",
            "Collecting tensorflow-privacy==0.9.0 (from tensorflow-federated)\n",
            "  Downloading tensorflow_privacy-0.9.0-py3-none-any.whl.metadata (763 bytes)\n",
            "Collecting tensorflow==2.14.*,>=2.14.0 (from tensorflow-federated)\n",
            "  Downloading tensorflow-2.14.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tqdm~=4.64 in /usr/local/lib/python3.11/dist-packages (from tensorflow-federated) (4.67.1)\n",
            "Collecting typing-extensions==4.5.*,>=4.5.0 (from tensorflow-federated)\n",
            "  Downloading typing_extensions-4.5.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting googleapis-common-protos==1.61.0 (from tensorflow-federated)\n",
            "  Downloading googleapis_common_protos-1.61.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: mpmath~=1.2 in /usr/local/lib/python3.11/dist-packages (from dp-accounting==0.4.3->tensorflow-federated) (1.3.0)\n",
            "Collecting attrs~=23.1 (from tensorflow-federated)\n",
            "  Downloading attrs-23.1.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: protobuf>=3.6 in /usr/local/lib/python3.11/dist-packages (from google-vizier==0.1.11->tensorflow-federated) (5.29.5)\n",
            "Collecting grpcio-tools>=1.35.0 (from google-vizier==0.1.11->tensorflow-federated)\n",
            "  Downloading grpcio_tools-1.76.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (5.3 kB)\n",
            "Collecting sqlalchemy<=1.4.20,>=1.4 (from google-vizier==0.1.11->tensorflow-federated)\n",
            "  Downloading SQLAlchemy-1.4.20.tar.gz (7.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m134.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting protobuf>=3.6 (from google-vizier==0.1.11->tensorflow-federated)\n",
            "  Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.11/dist-packages (from jax==0.4.14->tensorflow-federated) (3.4.0)\n",
            "Requirement already satisfied: six~=1.14 in /usr/local/lib/python3.11/dist-packages (from tensorflow-model-optimization==0.7.5->tensorflow-federated) (1.17.0)\n",
            "Collecting packaging~=22.0 (from tensorflow-privacy==0.9.0->tensorflow-federated)\n",
            "  Downloading packaging-22.0-py3-none-any.whl.metadata (3.1 kB)\n",
            "Requirement already satisfied: scikit-learn==1.*,>=1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow-privacy==0.9.0->tensorflow-federated) (1.6.1)\n",
            "Collecting tensorflow-estimator~=2.4 (from tensorflow-privacy==0.9.0->tensorflow-federated)\n",
            "  Downloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting tensorflow-probability~=0.22.0 (from tensorflow-privacy==0.9.0->tensorflow-federated)\n",
            "  Downloading tensorflow_probability-0.22.1-py2.py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from portpicker~=1.6->tensorflow-federated) (5.9.5)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.*,>=1.0->tensorflow-privacy==0.9.0->tensorflow-federated) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.*,>=1.0->tensorflow-privacy==0.9.0->tensorflow-federated) (3.6.0)\n",
            "Collecting numpy~=1.25 (from tensorflow-federated)\n",
            "  Downloading numpy-1.25.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy<=1.4.20,>=1.4->google-vizier==0.1.11->tensorflow-federated) (3.2.3)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.*,>=2.14.0->tensorflow-federated) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.*,>=2.14.0->tensorflow-federated) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.*,>=2.14.0->tensorflow-federated) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.*,>=2.14.0->tensorflow-federated) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.*,>=2.14.0->tensorflow-federated) (3.14.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.*,>=2.14.0->tensorflow-federated) (18.1.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.*,>=2.14.0->tensorflow-federated) (75.2.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.*,>=2.14.0->tensorflow-federated) (3.1.0)\n",
            "Collecting wrapt<1.15,>=1.11.0 (from tensorflow==2.14.*,>=2.14.0->tensorflow-federated)\n",
            "  Downloading wrapt-1.14.2-cp311-cp311-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.*,>=2.14.0->tensorflow-federated) (0.37.1)\n",
            "Collecting tensorboard<2.15,>=2.14 (from tensorflow==2.14.*,>=2.14.0->tensorflow-federated)\n",
            "  Downloading tensorboard-2.14.1-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting tensorflow-estimator~=2.4 (from tensorflow-privacy==0.9.0->tensorflow-federated)\n",
            "  Downloading tensorflow_estimator-2.14.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting keras<2.15,>=2.14.0 (from tensorflow==2.14.*,>=2.14.0->tensorflow-federated)\n",
            "  Downloading keras-2.14.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.*,>=2.14.0->tensorflow-federated) (2.38.0)\n",
            "Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard<2.15,>=2.14->tensorflow==2.14.*,>=2.14.0->tensorflow-federated)\n",
            "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.*,>=2.14.0->tensorflow-federated) (3.8.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.*,>=2.14.0->tensorflow-federated) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.*,>=2.14.0->tensorflow-federated) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.*,>=2.14.0->tensorflow-federated) (3.1.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14.*,>=2.14.0->tensorflow-federated) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14.*,>=2.14.0->tensorflow-federated) (4.9.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow==2.14.*,>=2.14.0->tensorflow-federated) (2.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow==2.14.*,>=2.14.0->tensorflow-federated) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow==2.14.*,>=2.14.0->tensorflow-federated) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow==2.14.*,>=2.14.0->tensorflow-federated) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow==2.14.*,>=2.14.0->tensorflow-federated) (2025.7.14)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.11/dist-packages (from rsa<5,>=3.1.4->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14.*,>=2.14.0->tensorflow-federated) (0.6.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from tensorflow-probability~=0.22.0->tensorflow-privacy==0.9.0->tensorflow-federated) (4.4.2)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow-probability~=0.22.0->tensorflow-privacy==0.9.0->tensorflow-federated) (3.1.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow==2.14.*,>=2.14.0->tensorflow-federated) (0.45.1)\n",
            "INFO: pip is looking at multiple versions of grpcio-tools to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting grpcio-tools>=1.35.0 (from google-vizier==0.1.11->tensorflow-federated)\n",
            "  Downloading grpcio_tools-1.75.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (5.3 kB)\n",
            "  Downloading grpcio_tools-1.75.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (5.3 kB)\n",
            "  Downloading grpcio_tools-1.74.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "  Downloading grpcio_tools-1.73.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "  Downloading grpcio_tools-1.73.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "  Downloading grpcio_tools-1.72.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "  Downloading grpcio_tools-1.72.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "INFO: pip is still looking at multiple versions of grpcio-tools to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading grpcio_tools-1.71.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "  Downloading grpcio_tools-1.71.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "  Downloading grpcio_tools-1.70.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "  Downloading grpcio_tools-1.69.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "  Downloading grpcio_tools-1.68.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading grpcio_tools-1.68.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "  Downloading grpcio_tools-1.67.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "  Downloading grpcio_tools-1.67.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "  Downloading grpcio_tools-1.66.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "  Downloading grpcio_tools-1.66.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "  Downloading grpcio_tools-1.66.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "  Downloading grpcio_tools-1.65.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "  Downloading grpcio_tools-1.65.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "  Downloading grpcio_tools-1.65.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "  Downloading grpcio_tools-1.65.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "  Downloading grpcio_tools-1.64.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "  Downloading grpcio_tools-1.64.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "  Downloading grpcio_tools-1.64.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "  Downloading grpcio_tools-1.63.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "  Downloading grpcio_tools-1.63.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "  Downloading grpcio_tools-1.62.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow==2.14.*,>=2.14.0->tensorflow-federated) (3.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow==2.14.*,>=2.14.0->tensorflow-federated) (3.0.2)\n",
            "Downloading tensorflow_federated-0.87.0-py3-none-manylinux_2_31_x86_64.whl (71.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 MB\u001b[0m \u001b[31m69.1 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading dm_tree-0.1.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (152 kB)\n",
            "Downloading dp_accounting-0.4.3-py3-none-any.whl (104 kB)\n",
            "Downloading google_vizier-0.1.11-py3-none-any.whl (721 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m721.6/721.6 kB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading attrs-23.1.0-py3-none-any.whl (61 kB)\n",
            "Downloading googleapis_common_protos-1.61.0-py2.py3-none-any.whl (230 kB)\n",
            "Downloading jaxlib-0.4.14-cp311-cp311-manylinux2014_x86_64.whl (73.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.7/73.7 MB\u001b[0m \u001b[31m55.7 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_model_optimization-0.7.5-py2.py3-none-any.whl (241 kB)\n",
            "Downloading tensorflow_privacy-0.9.0-py3-none-any.whl (323 kB)\n",
            "Downloading ml_dtypes-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m77.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading packaging-22.0-py3-none-any.whl (42 kB)\n",
            "Downloading portpicker-1.6.0-py3-none-any.whl (16 kB)\n",
            "Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "Downloading scipy-1.9.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.4/33.4 MB\u001b[0m \u001b[31m81.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.25.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m136.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow-2.14.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (489.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m489.9/489.9 MB\u001b[0m \u001b[31m55.5 MB/s\u001b[0m  \u001b[33m0:00:06\u001b[0m\n",
            "\u001b[?25hDownloading keras-2.14.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m56.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.14.1-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m157.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
            "Downloading tensorflow_estimator-2.14.0-py2.py3-none-any.whl (440 kB)\n",
            "Downloading tensorflow_probability-0.22.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m195.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
            "Downloading wrapt-1.14.2-cp311-cp311-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (77 kB)\n",
            "Downloading grpcio_tools-1.62.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m158.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: jax, sqlalchemy\n",
            "  Building wheel for jax (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jax: filename=jax-0.4.14-py3-none-any.whl size=1535471 sha256=3825bd1ab53a1682c7100fdcf02b462ac863454c4de46c001aed99bbc3188f91\n",
            "  Stored in directory: /root/.cache/pip/wheels/c4/8d/5d/66b1fbb551b0c3a21696015b7339b8241ebfa128bb9145febd\n",
            "  Building wheel for sqlalchemy (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sqlalchemy: filename=sqlalchemy-1.4.20-cp311-cp311-linux_x86_64.whl size=1531776 sha256=ede4f56fbe9075e5966ffb16fd9f7ce84b6b737df644eaa690ff196986d4beb0\n",
            "  Stored in directory: /root/.cache/pip/wheels/75/a2/a2/9290912f40321acfd65cf7a741d2e0882e9a1f9464f60e2e9b\n",
            "Successfully built jax sqlalchemy\n",
            "Installing collected packages: dm-tree, wrapt, typing-extensions, tensorflow-estimator, sqlalchemy, protobuf, portpicker, packaging, numpy, keras, attrs, tensorflow-probability, tensorflow-model-optimization, scipy, ml-dtypes, grpcio-tools, googleapis-common-protos, jaxlib, jax, google-vizier, google-auth-oauthlib, dp-accounting, tensorboard, tensorflow, tensorflow-privacy, tensorflow-federated\n",
            "\u001b[2K  Attempting uninstall: dm-tree\n",
            "\u001b[2K    Found existing installation: dm-tree 0.1.9\n",
            "\u001b[2K    Uninstalling dm-tree-0.1.9:\n",
            "\u001b[2K      Successfully uninstalled dm-tree-0.1.9\n",
            "\u001b[2K  Attempting uninstall: wrapt\n",
            "\u001b[2K    Found existing installation: wrapt 1.17.2\n",
            "\u001b[2K    Uninstalling wrapt-1.17.2:\n",
            "\u001b[2K      Successfully uninstalled wrapt-1.17.2\n",
            "\u001b[2K  Attempting uninstall: typing-extensions\n",
            "\u001b[2K    Found existing installation: typing_extensions 4.14.1\n",
            "\u001b[2K    Uninstalling typing_extensions-4.14.1:\n",
            "\u001b[2K      Successfully uninstalled typing_extensions-4.14.1\n",
            "\u001b[2K  Attempting uninstall: sqlalchemy\n",
            "\u001b[2K    Found existing installation: SQLAlchemy 2.0.41\n",
            "\u001b[2K    Uninstalling SQLAlchemy-2.0.41:\n",
            "\u001b[2K      Successfully uninstalled SQLAlchemy-2.0.41\n",
            "\u001b[2K  Attempting uninstall: protobuf\n",
            "\u001b[2K    Found existing installation: protobuf 5.29.5\n",
            "\u001b[2K    Uninstalling protobuf-5.29.5:\n",
            "\u001b[2K      Successfully uninstalled protobuf-5.29.5\n",
            "\u001b[2K  Attempting uninstall: portpicker\n",
            "\u001b[2K    Found existing installation: portpicker 1.5.2\n",
            "\u001b[2K    Uninstalling portpicker-1.5.2:\n",
            "\u001b[2K      Successfully uninstalled portpicker-1.5.2\n",
            "\u001b[2K  Attempting uninstall: packaging\n",
            "\u001b[2K    Found existing installation: packaging 24.2\n",
            "\u001b[2K    Uninstalling packaging-24.2:\n",
            "\u001b[2K      Successfully uninstalled packaging-24.2\n",
            "\u001b[2K  Attempting uninstall: numpy\n",
            "\u001b[2K    Found existing installation: numpy 2.0.2\n",
            "\u001b[2K    Uninstalling numpy-2.0.2:\n",
            "\u001b[2K      Successfully uninstalled numpy-2.0.2\n",
            "\u001b[2K  Attempting uninstall: keras\n",
            "\u001b[2K    Found existing installation: keras 3.8.0\n",
            "\u001b[2K    Uninstalling keras-3.8.0:\n",
            "\u001b[2K      Successfully uninstalled keras-3.8.0\n",
            "\u001b[2K  Attempting uninstall: attrs\n",
            "\u001b[2K    Found existing installation: attrs 25.3.0\n",
            "\u001b[2K    Uninstalling attrs-25.3.0:\n",
            "\u001b[2K      Successfully uninstalled attrs-25.3.0\n",
            "\u001b[2K  Attempting uninstall: tensorflow-probability\n",
            "\u001b[2K    Found existing installation: tensorflow-probability 0.25.0\n",
            "\u001b[2K    Uninstalling tensorflow-probability-0.25.0:\n",
            "\u001b[2K      Successfully uninstalled tensorflow-probability-0.25.0\n",
            "\u001b[2K  Attempting uninstall: scipy\n",
            "\u001b[2K    Found existing installation: scipy 1.15.3\n",
            "\u001b[2K    Uninstalling scipy-1.15.3:\n",
            "\u001b[2K      Successfully uninstalled scipy-1.15.3\n",
            "\u001b[2K  Attempting uninstall: ml-dtypes\n",
            "\u001b[2K    Found existing installation: ml-dtypes 0.4.1\n",
            "\u001b[2K    Uninstalling ml-dtypes-0.4.1:\n",
            "\u001b[2K      Successfully uninstalled ml-dtypes-0.4.1\n",
            "\u001b[2K  Attempting uninstall: googleapis-common-protos\n",
            "\u001b[2K    Found existing installation: googleapis-common-protos 1.70.0\n",
            "\u001b[2K    Uninstalling googleapis-common-protos-1.70.0:\n",
            "\u001b[2K      Successfully uninstalled googleapis-common-protos-1.70.0\n",
            "\u001b[2K  Attempting uninstall: jaxlib\n",
            "\u001b[2K    Found existing installation: jaxlib 0.5.1\n",
            "\u001b[2K    Uninstalling jaxlib-0.5.1:\n",
            "\u001b[2K      Successfully uninstalled jaxlib-0.5.1\n",
            "\u001b[2K  Attempting uninstall: jax\n",
            "\u001b[2K    Found existing installation: jax 0.5.2\n",
            "\u001b[2K    Uninstalling jax-0.5.2:\n",
            "\u001b[2K      Successfully uninstalled jax-0.5.2\n",
            "\u001b[2K  Attempting uninstall: google-auth-oauthlib\n",
            "\u001b[2K    Found existing installation: google-auth-oauthlib 1.2.2\n",
            "\u001b[2K    Uninstalling google-auth-oauthlib-1.2.2:\n",
            "\u001b[2K      Successfully uninstalled google-auth-oauthlib-1.2.2\n",
            "\u001b[2K  Attempting uninstall: tensorboard\n",
            "\u001b[2K    Found existing installation: tensorboard 2.18.0\n",
            "\u001b[2K    Uninstalling tensorboard-2.18.0:\n",
            "\u001b[2K      Successfully uninstalled tensorboard-2.18.0\n",
            "\u001b[2K  Attempting uninstall: tensorflow\n",
            "\u001b[2K    Found existing installation: tensorflow 2.18.0\n",
            "\u001b[2K    Uninstalling tensorflow-2.18.0:\n",
            "\u001b[2K      Successfully uninstalled tensorflow-2.18.0\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26/26\u001b[0m [tensorflow-federated]\n",
            "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires portpicker==1.5.2, but you have portpicker 1.6.0 which is incompatible.\n",
            "albumentations 2.0.8 requires scipy>=1.10.0, but you have scipy 1.9.3 which is incompatible.\n",
            "altair 5.5.0 requires typing-extensions>=4.10.0; python_version < \"3.14\", but you have typing-extensions 4.5.0 which is incompatible.\n",
            "blosc2 3.5.1 requires numpy>=1.26, but you have numpy 1.25.2 which is incompatible.\n",
            "chex 0.1.89 requires jax>=0.4.27, but you have jax 0.4.14 which is incompatible.\n",
            "chex 0.1.89 requires jaxlib>=0.4.27, but you have jaxlib 0.4.14 which is incompatible.\n",
            "cvxpy 1.6.6 requires scipy>=1.11.0, but you have scipy 1.9.3 which is incompatible.\n",
            "db-dtypes 1.4.3 requires packaging>=24.2.0, but you have packaging 22.0 which is incompatible.\n",
            "fastapi 0.116.1 requires typing-extensions>=4.8.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "flax 0.10.6 requires jax>=0.5.1, but you have jax 0.4.14 which is incompatible.\n",
            "google-cloud-bigquery 3.34.0 requires packaging>=24.2.0, but you have packaging 22.0 which is incompatible.\n",
            "google-genai 1.25.0 requires typing-extensions<5.0.0,>=4.11.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.8 which is incompatible.\n",
            "imbalanced-learn 0.13.0 requires scipy<2,>=1.10.1, but you have scipy 1.9.3 which is incompatible.\n",
            "ipython-sql 0.5.0 requires sqlalchemy>=2.0, but you have sqlalchemy 1.4.20 which is incompatible.\n",
            "langchain-core 0.3.68 requires packaging<25,>=23.2, but you have packaging 22.0 which is incompatible.\n",
            "langchain-core 0.3.68 requires typing-extensions>=4.7, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "langsmith 0.4.5 requires packaging>=23.2, but you have packaging 22.0 which is incompatible.\n",
            "nibabel 5.3.2 requires typing-extensions>=4.6; python_version < \"3.13\", but you have typing-extensions 4.5.0 which is incompatible.\n",
            "openai 1.95.1 requires typing-extensions<5,>=4.11, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.25.2 which is incompatible.\n",
            "optax 0.2.5 requires jax>=0.4.27, but you have jax 0.4.14 which is incompatible.\n",
            "optax 0.2.5 requires jaxlib>=0.4.27, but you have jaxlib 0.4.14 which is incompatible.\n",
            "optree 0.16.0 requires typing-extensions>=4.6.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "orbax-checkpoint 0.11.16 requires jax>=0.5.0, but you have jax 0.4.14 which is incompatible.\n",
            "pydantic 2.11.7 requires typing-extensions>=4.12.2, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "pydantic-core 2.33.2 requires typing-extensions!=4.7.0,>=4.6.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "scikit-image 0.25.2 requires scipy>=1.11.4, but you have scipy 1.9.3 which is incompatible.\n",
            "sphinx 8.2.3 requires packaging>=23.0, but you have packaging 22.0 which is incompatible.\n",
            "starlette 0.47.1 requires typing-extensions>=4.10.0; python_version < \"3.13\", but you have typing-extensions 4.5.0 which is incompatible.\n",
            "stumpy 1.13.0 requires scipy>=1.10, but you have scipy 1.9.3 which is incompatible.\n",
            "tensorflow-decision-forests 1.11.0 requires tensorflow==2.18.0, but you have tensorflow 2.14.1 which is incompatible.\n",
            "tensorflow-text 2.18.1 requires tensorflow<2.19,>=2.18.0, but you have tensorflow 2.14.1 which is incompatible.\n",
            "tensorstore 0.1.74 requires ml_dtypes>=0.3.1, but you have ml-dtypes 0.2.0 which is incompatible.\n",
            "tf-keras 2.18.0 requires tensorflow<2.19,>=2.18, but you have tensorflow 2.14.1 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.25.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires typing-extensions>=4.10.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.9.3 which is incompatible.\n",
            "typeguard 4.4.4 requires typing_extensions>=4.14.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "typing-inspection 0.4.1 requires typing-extensions>=4.12.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "wandb 0.21.0 requires typing-extensions<5,>=4.8, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "xarray 2025.3.1 requires packaging>=23.2, but you have packaging 22.0 which is incompatible.\n",
            "xarray-einstats 0.9.1 requires scipy>=1.11, but you have scipy 1.9.3 which is incompatible.\n",
            "ydf 0.12.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 4.25.8 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed attrs-23.1.0 dm-tree-0.1.8 dp-accounting-0.4.3 google-auth-oauthlib-1.0.0 google-vizier-0.1.11 googleapis-common-protos-1.61.0 grpcio-tools-1.62.3 jax-0.4.14 jaxlib-0.4.14 keras-2.14.0 ml-dtypes-0.2.0 numpy-1.25.2 packaging-22.0 portpicker-1.6.0 protobuf-4.25.8 scipy-1.9.3 sqlalchemy-1.4.20 tensorboard-2.14.1 tensorflow-2.14.1 tensorflow-estimator-2.14.0 tensorflow-federated-0.87.0 tensorflow-model-optimization-0.7.5 tensorflow-privacy-0.9.0 tensorflow-probability-0.22.1 typing-extensions-4.5.0 wrapt-1.14.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google",
                  "numpy",
                  "packaging",
                  "portpicker"
                ]
              },
              "id": "d120d9d365ae4f1b825cbe7fbcc5074a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wfdb==4.1.0\n",
            "  Downloading wfdb-4.1.0-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting SoundFile<0.12.0,>=0.10.0 (from wfdb==4.1.0)\n",
            "  Downloading soundfile-0.11.0-py2.py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: matplotlib<4.0.0,>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from wfdb==4.1.0) (3.10.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.10.1 in /usr/local/lib/python3.11/dist-packages (from wfdb==4.1.0) (1.25.2)\n",
            "Collecting pandas<2.0.0,>=1.0.0 (from wfdb==4.1.0)\n",
            "  Downloading pandas-1.5.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.8.1 in /usr/local/lib/python3.11/dist-packages (from wfdb==4.1.0) (2.32.3)\n",
            "Requirement already satisfied: scipy<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wfdb==4.1.0) (1.9.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4.0.0,>=3.2.2->wfdb==4.1.0) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4.0.0,>=3.2.2->wfdb==4.1.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4.0.0,>=3.2.2->wfdb==4.1.0) (4.58.5)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4.0.0,>=3.2.2->wfdb==4.1.0) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4.0.0,>=3.2.2->wfdb==4.1.0) (22.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4.0.0,>=3.2.2->wfdb==4.1.0) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4.0.0,>=3.2.2->wfdb==4.1.0) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4.0.0,>=3.2.2->wfdb==4.1.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<2.0.0,>=1.0.0->wfdb==4.1.0) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.8.1->wfdb==4.1.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.8.1->wfdb==4.1.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.8.1->wfdb==4.1.0) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.8.1->wfdb==4.1.0) (2025.7.14)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from SoundFile<0.12.0,>=0.10.0->wfdb==4.1.0) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->SoundFile<0.12.0,>=0.10.0->wfdb==4.1.0) (2.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib<4.0.0,>=3.2.2->wfdb==4.1.0) (1.17.0)\n",
            "Downloading wfdb-4.1.0-py3-none-any.whl (159 kB)\n",
            "Downloading pandas-1.5.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m180.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading soundfile-0.11.0-py2.py3-none-any.whl (23 kB)\n",
            "Installing collected packages: SoundFile, pandas, wfdb\n",
            "\u001b[2K  Attempting uninstall: SoundFile\n",
            "\u001b[2K    Found existing installation: soundfile 0.13.1\n",
            "\u001b[2K    Uninstalling soundfile-0.13.1:\n",
            "\u001b[2K      Successfully uninstalled soundfile-0.13.1\n",
            "\u001b[2K  Attempting uninstall: pandas\n",
            "\u001b[2K    Found existing installation: pandas 2.2.2\n",
            "\u001b[2K    Uninstalling pandas-2.2.2:\n",
            "\u001b[2K      Successfully uninstalled pandas-2.2.2\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [wfdb]\n",
            "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 1.5.3 which is incompatible.\n",
            "google-colab 1.0.0 requires portpicker==1.5.2, but you have portpicker 1.6.0 which is incompatible.\n",
            "cudf-cu12 25.2.1 requires pandas<2.2.4dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "dask-cudf-cu12 25.2.2 requires pandas<2.2.4dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "dask-expr 1.1.21 requires pandas>=2, but you have pandas 1.5.3 which is incompatible.\n",
            "db-dtypes 1.4.3 requires packaging>=24.2.0, but you have packaging 22.0 which is incompatible.\n",
            "librosa 0.11.0 requires soundfile>=0.12.1, but you have soundfile 0.11.0 which is incompatible.\n",
            "mizani 0.13.5 requires pandas>=2.2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "plotnine 0.14.6 requires pandas>=2.2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "tensorflow-decision-forests 1.11.0 requires tensorflow==2.18.0, but you have tensorflow 2.14.1 which is incompatible.\n",
            "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.9.3 which is incompatible.\n",
            "xarray 2025.3.1 requires packaging>=23.2, but you have packaging 22.0 which is incompatible.\n",
            "xarray 2025.3.1 requires pandas>=2.1, but you have pandas 1.5.3 which is incompatible.\n",
            "xarray-einstats 0.9.1 requires scipy>=1.11, but you have scipy 1.9.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed SoundFile-0.11.0 pandas-1.5.3 wfdb-4.1.0\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.25.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.1->pandas) (1.17.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.9.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.11/dist-packages (0.13.0)\n",
            "Requirement already satisfied: numpy<3,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (1.25.2)\n",
            "Collecting scipy<2,>=1.10.1 (from imbalanced-learn)\n",
            "  Downloading scipy-1.16.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (62 kB)\n",
            "Requirement already satisfied: scikit-learn<2,>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (1.6.1)\n",
            "Requirement already satisfied: sklearn-compat<1,>=0.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (0.1.3)\n",
            "Requirement already satisfied: joblib<2,>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (3.6.0)\n",
            "Downloading scipy-1.16.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.9/35.9 MB\u001b[0m \u001b[31m87.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: scipy\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.9.3\n",
            "    Uninstalling scipy-1.9.3:\n",
            "      Successfully uninstalled scipy-1.9.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-federated 0.87.0 requires scipy~=1.9.3, but you have scipy 1.16.3 which is incompatible.\n",
            "chex 0.1.89 requires jax>=0.4.27, but you have jax 0.4.14 which is incompatible.\n",
            "chex 0.1.89 requires jaxlib>=0.4.27, but you have jaxlib 0.4.14 which is incompatible.\n",
            "flax 0.10.6 requires jax>=0.5.1, but you have jax 0.4.14 which is incompatible.\n",
            "librosa 0.11.0 requires soundfile>=0.12.1, but you have soundfile 0.11.0 which is incompatible.\n",
            "mizani 0.13.5 requires pandas>=2.2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "optax 0.2.5 requires jax>=0.4.27, but you have jax 0.4.14 which is incompatible.\n",
            "optax 0.2.5 requires jaxlib>=0.4.27, but you have jaxlib 0.4.14 which is incompatible.\n",
            "orbax-checkpoint 0.11.16 requires jax>=0.5.0, but you have jax 0.4.14 which is incompatible.\n",
            "plotnine 0.14.6 requires pandas>=2.2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "plotnine 0.14.6 requires scipy<1.16.0,>=1.8.0, but you have scipy 1.16.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed scipy-1.16.3\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.5)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (22.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /usr/local/lib/python3.11/dist-packages (from seaborn) (1.25.2)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.11/dist-packages (from seaborn) (1.5.3)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/local/lib/python3.11/dist-packages (from seaborn) (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.58.5)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (22.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n",
            "✅ All packages installed successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from collections import Counter\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import os\n",
        "import wfdb\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "print(\"🎯 HIGH-PERFORMANCE FEDERATED ECG ANALYSIS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Mount Google Drive and set the correct path\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set the correct data path in your Google Drive\n",
        "DATA_PATH = \"/content/drive/MyDrive/mit-bih-arrhythmia-database\"\n",
        "\n",
        "class HighPerformanceFederatedECG:\n",
        "    def __init__(self, num_classes=4):\n",
        "        self.num_classes = num_classes\n",
        "        self.class_weights = None\n",
        "\n",
        "    def balanced_performance_loss(self, y_true, y_pred):\n",
        "        \"\"\"Improved loss function for better training stability\"\"\"\n",
        "        y_true = tf.cast(y_true, tf.float32)\n",
        "        y_pred = tf.cast(y_pred, tf.float32)\n",
        "\n",
        "        # Standard cross entropy with label smoothing for stability\n",
        "        ce = tf.keras.losses.categorical_crossentropy(y_true, y_pred, from_logits=False, label_smoothing=0.1)\n",
        "\n",
        "        return tf.reduce_mean(ce)\n",
        "\n",
        "    def compute_performance_weights(self, y_train):\n",
        "        \"\"\"Compute balanced weights for all classes\"\"\"\n",
        "        if len(y_train.shape) > 1 and y_train.shape[1] > 1:\n",
        "            y_labels = np.argmax(y_train, axis=1)\n",
        "        else:\n",
        "            y_labels = y_train\n",
        "\n",
        "        class_counts = np.bincount(y_labels, minlength=self.num_classes)\n",
        "        total_samples = np.sum(class_counts)\n",
        "\n",
        "        print(\"📊 Class Distribution:\")\n",
        "        class_names = ['N', 'S', 'V', 'F']\n",
        "        for i, (name, count) in enumerate(zip(class_names, class_counts)):\n",
        "            print(f\"   {name}: {count:4d} samples ({count/total_samples*100:.1f}%)\")\n",
        "\n",
        "        # More balanced weight computation\n",
        "        weights = total_samples / (self.num_classes * class_counts)\n",
        "\n",
        "        # Normalize weights to prevent extreme values\n",
        "        weights = weights / np.max(weights)\n",
        "        weights = np.clip(weights, 0.5, 2.0)\n",
        "\n",
        "        self.class_weights = tf.constant(weights, dtype=tf.float32)\n",
        "\n",
        "        print(\"⚖️ Balanced Weights:\")\n",
        "        for i, (name, count, weight) in enumerate(zip(class_names, class_counts, weights)):\n",
        "            print(f\"   {name}: {count:4d} samples | Weight: {weight:.2f}\")\n",
        "\n",
        "        return weights\n",
        "\n",
        "    def add_performance_differential_privacy(self, gradients, noise_scale=0.001, clip_value=1.0):\n",
        "        \"\"\"Reduced DP for better learning\"\"\"\n",
        "        clipped_gradients = []\n",
        "\n",
        "        for grad in gradients:\n",
        "            if grad is not None:\n",
        "                # Gentle clipping\n",
        "                grad_norm = tf.norm(grad)\n",
        "                if grad_norm > clip_value:\n",
        "                    grad = grad * (clip_value / (grad_norm + 1e-8))\n",
        "\n",
        "                # Minimal noise\n",
        "                noise = tf.random.normal(\n",
        "                    shape=grad.shape,\n",
        "                    mean=0.0,\n",
        "                    stddev=noise_scale\n",
        "                )\n",
        "                grad = grad + noise\n",
        "\n",
        "            clipped_gradients.append(grad)\n",
        "\n",
        "        return clipped_gradients\n",
        "\n",
        "    def create_high_performance_model(self, input_shape=(187, 1)):\n",
        "        \"\"\"Simplified and more effective model architecture\"\"\"\n",
        "        model = models.Sequential([\n",
        "            layers.Input(shape=input_shape),\n",
        "\n",
        "            # First conv block\n",
        "            layers.Conv1D(32, 7, activation='relu', padding='same'),\n",
        "            layers.BatchNormalization(),\n",
        "            layers.MaxPooling1D(2),\n",
        "            layers.Dropout(0.2),\n",
        "\n",
        "            # Second conv block\n",
        "            layers.Conv1D(64, 5, activation='relu', padding='same'),\n",
        "            layers.BatchNormalization(),\n",
        "            layers.MaxPooling1D(2),\n",
        "            layers.Dropout(0.3),\n",
        "\n",
        "            # Third conv block\n",
        "            layers.Conv1D(128, 3, activation='relu', padding='same'),\n",
        "            layers.BatchNormalization(),\n",
        "            layers.GlobalAveragePooling1D(),\n",
        "            layers.Dropout(0.4),\n",
        "\n",
        "            # Dense layers\n",
        "            layers.Dense(64, activation='relu'),\n",
        "            layers.BatchNormalization(),\n",
        "            layers.Dropout(0.3),\n",
        "\n",
        "            layers.Dense(32, activation='relu'),\n",
        "            layers.Dropout(0.2),\n",
        "\n",
        "            # Output layer\n",
        "            layers.Dense(self.num_classes, activation='softmax')\n",
        "        ])\n",
        "\n",
        "        return model\n",
        "\n",
        "def load_real_ecg_data():\n",
        "    \"\"\"Load real ECG data from Google Drive\"\"\"\n",
        "    global DATA_PATH\n",
        "\n",
        "    print(\"📁 Loading Real MIT-BIH ECG Data...\")\n",
        "    print(f\"📂 Data path: {DATA_PATH}\")\n",
        "\n",
        "    # Check if data path exists\n",
        "    if not os.path.exists(DATA_PATH):\n",
        "        print(f\"❌ Data path not found: {DATA_PATH}\")\n",
        "        print(\"🔍 Searching for MIT-BIH data in Google Drive...\")\n",
        "\n",
        "        # Search for common MIT-BIH folder names\n",
        "        search_paths = [\n",
        "            \"/content/drive/MyDrive/mit-bih-arrhythmia-database\",\n",
        "            \"/content/drive/MyDrive/mit-bih-arrhythmia-database-1.0.0\",\n",
        "            \"/content/drive/MyDrive/mitdb\",\n",
        "            \"/content/drive/MyDrive/ECG_Data\",\n",
        "            \"/content/drive/MyDrive/ECG\",\n",
        "            \"/content/drive/MyDrive/mit-bih\",\n",
        "            \"/content/drive/MyDrive/arrhythmia-database\"\n",
        "        ]\n",
        "\n",
        "        found_path = None\n",
        "        for path in search_paths:\n",
        "            if os.path.exists(path):\n",
        "                found_path = path\n",
        "                DATA_PATH = path\n",
        "                print(f\"✅ Found data at: {DATA_PATH}\")\n",
        "                break\n",
        "\n",
        "        if not found_path:\n",
        "            print(\"❌ No MIT-BIH data found in Google Drive.\")\n",
        "            print(\"📥 Please download the MIT-BIH dataset:\")\n",
        "            print(\"   1. Go to: https://physionet.org/content/mitdb/1.0.0/\")\n",
        "            print(\"   2. Download the dataset\")\n",
        "            print(\"   3. Upload it to your Google Drive\")\n",
        "            print(\"   4. Ensure it's in: /content/drive/MyDrive/mit-bih-arrhythmia-database\")\n",
        "            return generate_synthetic_data()\n",
        "\n",
        "    try:\n",
        "        files = os.listdir(DATA_PATH)\n",
        "        print(f\"📄 Found {len(files)} files in directory\")\n",
        "\n",
        "        # Show ECG files\n",
        "        ecg_files = [f for f in files if f.endswith(('.hea', '.dat'))]\n",
        "        print(f\"📄 ECG files found: {len(ecg_files)}\")\n",
        "        if ecg_files:\n",
        "            print(f\"📄 Sample files: {sorted(ecg_files)[:10]}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Could not list directory: {e}\")\n",
        "        return generate_synthetic_data()\n",
        "\n",
        "    # Use a subset of records for faster testing\n",
        "    record_numbers = [100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 111, 112, 113, 114]\n",
        "\n",
        "    # Beat annotations according to AAMI standards\n",
        "    beat_codes = {\n",
        "        'N': ['N', 'L', 'R', 'e', 'j'],  # Normal beats\n",
        "        'S': ['A', 'a', 'S', 'J'],       # Supraventricular ectopic beats\n",
        "        'V': ['V', 'E'],                 # Ventricular ectopic beats\n",
        "        'F': ['F']                       # Fusion beats\n",
        "    }\n",
        "\n",
        "    # Reverse mapping for code to class\n",
        "    code_to_class = {}\n",
        "    for class_name, codes in beat_codes.items():\n",
        "        for code in codes:\n",
        "            code_to_class[code] = class_name\n",
        "\n",
        "    X = []\n",
        "    y = []\n",
        "\n",
        "    samples_collected = 0\n",
        "    records_loaded = 0\n",
        "\n",
        "    for record_num in record_numbers:\n",
        "        try:\n",
        "            # Try different file naming patterns\n",
        "            record_patterns = [\n",
        "                str(record_num),\n",
        "                f\"{record_num:03d}\",\n",
        "                f\"{record_num:05d}\"\n",
        "            ]\n",
        "\n",
        "            record_found = False\n",
        "            record_path = None\n",
        "\n",
        "            for pattern in record_patterns:\n",
        "                test_path = os.path.join(DATA_PATH, pattern)\n",
        "                if os.path.exists(test_path + '.hea'):\n",
        "                    record_path = test_path\n",
        "                    record_found = True\n",
        "                    break\n",
        "\n",
        "            if not record_found:\n",
        "                print(f\"⚠️ Record {record_num} not found, skipping...\")\n",
        "                continue\n",
        "\n",
        "            print(f\"📖 Loading record {record_num} from {record_path}...\")\n",
        "\n",
        "            # Read record and annotations\n",
        "            record = wfdb.rdrecord(record_path)\n",
        "            annotation = wfdb.rdann(record_path, 'atr')\n",
        "\n",
        "            # Get ECG signal (use first lead)\n",
        "            if record.p_signal is not None:\n",
        "                if len(record.p_signal.shape) > 1:\n",
        "                    signal = record.p_signal[:, 0]  # Use first channel\n",
        "                else:\n",
        "                    signal = record.p_signal\n",
        "            else:\n",
        "                print(f\"⚠️ No signal data in record {record_num}, skipping...\")\n",
        "                continue\n",
        "\n",
        "            # Extract beats\n",
        "            beat_count = 0\n",
        "            for i, sample in enumerate(annotation.sample):\n",
        "                symbol = annotation.symbol[i]\n",
        "\n",
        "                if symbol in code_to_class and sample > 90 and sample < len(signal) - 97:\n",
        "                    # Extract 187-sample window around the beat\n",
        "                    start = sample - 90   # 90 samples before\n",
        "                    end = sample + 97     # 97 samples after (total 187)\n",
        "\n",
        "                    if start >= 0 and end <= len(signal):\n",
        "                        beat = signal[start:end]\n",
        "\n",
        "                        # Skip if beat contains NaN or Inf\n",
        "                        if np.any(np.isnan(beat)) or np.any(np.isinf(beat)):\n",
        "                            continue\n",
        "\n",
        "                        # Normalize the beat\n",
        "                        beat_normalized = (beat - np.mean(beat)) / (np.std(beat) + 1e-8)\n",
        "\n",
        "                        X.append(beat_normalized)\n",
        "                        y.append(code_to_class[symbol])\n",
        "                        beat_count += 1\n",
        "                        samples_collected += 1\n",
        "\n",
        "            records_loaded += 1\n",
        "            print(f\"   ✅ Record {record_num}: {beat_count} beats\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Could not load record {record_num}: {e}\")\n",
        "            continue\n",
        "\n",
        "    if samples_collected == 0:\n",
        "        print(\"❌ No beats collected from real data! Check file format and permissions.\")\n",
        "        return generate_synthetic_data()\n",
        "\n",
        "    print(f\"✅ Collected {len(X)} beats from {records_loaded} MIT-BIH records\")\n",
        "\n",
        "    # Encode labels\n",
        "    label_encoder = LabelEncoder()\n",
        "    y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "    # Convert to one-hot\n",
        "    y_categorical = to_categorical(y_encoded)\n",
        "\n",
        "    # Reshape X for CNN\n",
        "    X_array = np.array(X).reshape(-1, 187, 1)\n",
        "\n",
        "    print(\"📊 Final dataset shape:\", X_array.shape)\n",
        "    print(\"📊 Label distribution:\", Counter(y))\n",
        "\n",
        "    return X_array, y_categorical, label_encoder\n",
        "\n",
        "def generate_synthetic_data():\n",
        "    \"\"\"Generate realistic synthetic ECG data for testing\"\"\"\n",
        "    print(\"🔄 Generating realistic synthetic ECG data...\")\n",
        "\n",
        "    num_samples = 2000\n",
        "    sequence_length = 187\n",
        "    num_classes = 4\n",
        "\n",
        "    # Generate synthetic ECG-like waveforms\n",
        "    X = []\n",
        "    y = []\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        # Different waveform patterns for different classes\n",
        "        class_id = i % num_classes\n",
        "\n",
        "        if class_id == 0:  # Normal beat (N)\n",
        "            t = np.linspace(0, 4*np.pi, sequence_length)\n",
        "            signal = np.sin(t) + 0.3 * np.sin(3*t) + 0.1 * np.random.normal(0, 0.1, sequence_length)\n",
        "        elif class_id == 1:  # Supraventricular (S)\n",
        "            t = np.linspace(0, 4*np.pi, sequence_length)\n",
        "            signal = 0.7 * np.sin(t) + 0.5 * np.sin(5*t) + 0.2 * np.random.normal(0, 0.15, sequence_length)\n",
        "        elif class_id == 2:  # Ventricular (V)\n",
        "            t = np.linspace(0, 4*np.pi, sequence_length)\n",
        "            signal = 1.2 * np.sin(0.8*t) + 0.8 * np.sin(2*t) + 0.3 * np.random.normal(0, 0.2, sequence_length)\n",
        "        else:  # Fusion (F)\n",
        "            t = np.linspace(0, 4*np.pi, sequence_length)\n",
        "            signal = 0.9 * np.sin(t) + 0.6 * np.sin(2*t) + 0.4 * np.sin(4*t) + 0.2 * np.random.normal(0, 0.12, sequence_length)\n",
        "\n",
        "        # Normalize\n",
        "        signal = (signal - np.mean(signal)) / (np.std(signal) + 1e-8)\n",
        "        X.append(signal)\n",
        "        y.append(class_id)\n",
        "\n",
        "    # Convert to arrays\n",
        "    X_array = np.array(X).reshape(-1, sequence_length, 1)\n",
        "    y_array = to_categorical(y, num_classes)\n",
        "\n",
        "    # Create label encoder\n",
        "    label_encoder = LabelEncoder()\n",
        "    label_encoder.classes_ = np.array(['N', 'S', 'V', 'F'])\n",
        "\n",
        "    print(f\"✅ Generated {len(X)} synthetic ECG beats\")\n",
        "    print(\"📊 Label distribution:\", Counter(y))\n",
        "\n",
        "    return X_array, y_array, label_encoder\n",
        "\n",
        "def create_balanced_client_data(X_train, y_train, num_clients=3):\n",
        "    \"\"\"Split data across clients with balanced distribution\"\"\"\n",
        "    client_data = []\n",
        "\n",
        "    # Convert back to labels for stratification\n",
        "    y_labels = np.argmax(y_train, axis=1)\n",
        "\n",
        "    for i in range(num_clients):\n",
        "        # Take stratified samples for each client\n",
        "        X_client, _, y_client, _ = train_test_split(\n",
        "            X_train, y_train,\n",
        "            train_size=0.8/num_clients,\n",
        "            random_state=42 + i,\n",
        "            stratify=y_labels\n",
        "        )\n",
        "        client_data.append((X_client, y_client))\n",
        "        print(f\"   Client {i}: {X_client.shape[0]} samples\")\n",
        "\n",
        "    return client_data\n",
        "\n",
        "def run_high_performance_learning():\n",
        "    \"\"\"Run improved federated learning with better training strategy\"\"\"\n",
        "\n",
        "    print(\"\\n🎯 STARTING HIGH-PERFORMANCE FEDERATED TRAINING\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Load data\n",
        "    X, y, label_encoder = load_real_ecg_data()\n",
        "\n",
        "    # Performance-optimized data splitting\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42, stratify=np.argmax(y, axis=1)\n",
        "    )\n",
        "\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X_train, y_train, test_size=0.1, random_state=42, stratify=np.argmax(y_train, axis=1)\n",
        "    )\n",
        "\n",
        "    print(f\"📊 Training samples: {X_train.shape[0]}\")\n",
        "    print(f\"📊 Validation samples: {X_val.shape[0]}\")\n",
        "    print(f\"📊 Test samples: {X_test.shape[0]}\")\n",
        "\n",
        "    # Initialize FL system\n",
        "    num_classes = len(label_encoder.classes_)\n",
        "    fl_system = HighPerformanceFederatedECG(num_classes=num_classes)\n",
        "\n",
        "    # Compute performance-optimized weights\n",
        "    print(\"\\n⚖️ Computing performance-optimized weights...\")\n",
        "    class_weights = fl_system.compute_performance_weights(y_train)\n",
        "\n",
        "    # Create model\n",
        "    print(\"🧠 Creating high-performance model...\")\n",
        "    model = fl_system.create_high_performance_model()\n",
        "\n",
        "    # Compile with proper metrics\n",
        "    print(\"🔧 Compiling model for high performance...\")\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "        loss=fl_system.balanced_performance_loss,\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    # Client setup\n",
        "    print(\"\\n👥 Setting up 3 clients for high performance...\")\n",
        "    client_data = create_balanced_client_data(X_train, y_train, num_clients=3)\n",
        "\n",
        "    # Improved training parameters\n",
        "    rounds = 15\n",
        "    batch_size = 32\n",
        "\n",
        "    print(\"\\n🔄 Starting High-Performance Federated Training...\")\n",
        "    print(\"   - Gentle Differential Privacy: ✅ Enabled\")\n",
        "    print(\"   - Balanced Loss: ✅ Enabled\")\n",
        "    print(\"   - Class Weights: ✅ Enabled\")\n",
        "    print(\"   - Clients: 3\")\n",
        "    print(\"   - Rounds: 15\")\n",
        "    print(\"   - Target: Balanced Performance\")\n",
        "    print(f\"   - Classes: {num_classes} ({', '.join(label_encoder.classes_)})\")\n",
        "\n",
        "    global_weights = model.get_weights()\n",
        "    best_accuracy = 0\n",
        "    best_weights = None\n",
        "    best_round = 0\n",
        "\n",
        "    training_history = []\n",
        "\n",
        "    for round_num in range(rounds):\n",
        "        print(f\"\\n🎯 Round {round_num + 1}/{rounds}\")\n",
        "\n",
        "        client_weights = []\n",
        "        round_accuracies = []\n",
        "\n",
        "        for client_id, (X_client, y_client) in enumerate(client_data):\n",
        "            # Create client model\n",
        "            client_model = fl_system.create_high_performance_model()\n",
        "            client_model.set_weights(global_weights)\n",
        "\n",
        "            client_model.compile(\n",
        "                optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "                loss=fl_system.balanced_performance_loss,\n",
        "                metrics=['accuracy']\n",
        "            )\n",
        "\n",
        "            # Train\n",
        "            history = client_model.fit(\n",
        "                X_client, y_client,\n",
        "                batch_size=batch_size,\n",
        "                epochs=2,\n",
        "                verbose=0,\n",
        "                class_weight={i: weight.numpy() for i, weight in enumerate(fl_system.class_weights)}\n",
        "            )\n",
        "\n",
        "            final_accuracy = history.history['accuracy'][-1]\n",
        "            round_accuracies.append(final_accuracy)\n",
        "\n",
        "            # Get gradients with gentle DP\n",
        "            with tf.GradientTape() as tape:\n",
        "                predictions = client_model(X_client, training=True)\n",
        "                loss = fl_system.balanced_performance_loss(y_client, predictions)\n",
        "\n",
        "            gradients = tape.gradient(loss, client_model.trainable_variables)\n",
        "            dp_gradients = fl_system.add_performance_differential_privacy(gradients)\n",
        "            client_model.optimizer.apply_gradients(zip(dp_gradients, client_model.trainable_variables))\n",
        "\n",
        "            client_weights.append(client_model.get_weights())\n",
        "\n",
        "            if client_id == 0 and (round_num + 1) % 5 == 0:\n",
        "                print(f\"   Client {client_id}: Accuracy = {final_accuracy:.4f}\")\n",
        "\n",
        "        # Aggregate weights\n",
        "        print(\"   🔒 Aggregating client updates...\")\n",
        "        new_global_weights = []\n",
        "        for layer_weights in zip(*client_weights):\n",
        "            layer_avg = np.mean(layer_weights, axis=0)\n",
        "            new_global_weights.append(layer_avg)\n",
        "\n",
        "        global_weights = new_global_weights\n",
        "        model.set_weights(global_weights)\n",
        "\n",
        "        # Evaluate\n",
        "        test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "        # Detailed evaluation using sklearn\n",
        "        y_pred = model.predict(X_test, verbose=0)\n",
        "        y_pred_labels = np.argmax(y_pred, axis=1)\n",
        "        y_true_labels = np.argmax(y_test, axis=1)\n",
        "\n",
        "        from sklearn.metrics import precision_recall_fscore_support\n",
        "        precision, recall, f1, support = precision_recall_fscore_support(\n",
        "            y_true_labels, y_pred_labels, labels=range(num_classes), average=None, zero_division=0\n",
        "        )\n",
        "\n",
        "        avg_precision = np.mean(precision)\n",
        "        avg_recall = np.mean(recall)\n",
        "        avg_f1 = np.mean(f1)\n",
        "\n",
        "        print(f\"   📊 Test Accuracy: {test_accuracy:.4f}\")\n",
        "        print(f\"   🎯 Avg Precision: {avg_precision:.4f}, Avg Recall: {avg_recall:.4f}\")\n",
        "\n",
        "        training_history.append({\n",
        "            'round': round_num + 1,\n",
        "            'test_accuracy': test_accuracy,\n",
        "            'avg_precision': avg_precision,\n",
        "            'avg_recall': avg_recall,\n",
        "            'avg_f1': avg_f1\n",
        "        })\n",
        "\n",
        "        # Save best model\n",
        "        if test_accuracy > best_accuracy:\n",
        "            best_accuracy = test_accuracy\n",
        "            best_weights = model.get_weights()\n",
        "            best_round = round_num + 1\n",
        "            print(f\"   💫 New best accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "    # Use best model\n",
        "    model.set_weights(best_weights)\n",
        "\n",
        "    # Training progress analysis\n",
        "    print(f\"\\n📈 TRAINING PROGRESS ANALYSIS:\")\n",
        "    for progress in training_history[-5:]:\n",
        "        status = \"✅ GOOD\" if progress['test_accuracy'] > 0.7 else \"📊 DECENT\" if progress['test_accuracy'] > 0.5 else \"⚠️ NEEDS WORK\"\n",
        "        print(f\"   Round {progress['round']}: Acc={progress['test_accuracy']:.4f}, Prec={progress['avg_precision']:.4f} [{status}]\")\n",
        "\n",
        "    print(f\"\\n🏆 Best model from round {best_round}\")\n",
        "    print(f\"   Best Accuracy: {best_accuracy:.4f}\")\n",
        "\n",
        "    return model, fl_system, X_test, y_test, label_encoder\n",
        "\n",
        "def comprehensive_performance_evaluation(model, X_test, y_test, label_encoder):\n",
        "    \"\"\"Comprehensive evaluation with detailed performance metrics\"\"\"\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"📊 COMPREHENSIVE PERFORMANCE EVALUATION\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Predictions\n",
        "    y_pred = model.predict(X_test, verbose=0)\n",
        "    y_pred_labels = np.argmax(y_pred, axis=1)\n",
        "    y_true_labels = np.argmax(y_test, axis=1)\n",
        "\n",
        "    # Comprehensive metrics\n",
        "    from sklearn.metrics import precision_recall_fscore_support, classification_report, confusion_matrix\n",
        "\n",
        "    class_names = label_encoder.classes_\n",
        "\n",
        "    # Per-class metrics\n",
        "    precision, recall, f1, support = precision_recall_fscore_support(\n",
        "        y_true_labels, y_pred_labels, labels=range(len(class_names)), average=None, zero_division=0\n",
        "    )\n",
        "\n",
        "    # Overall metrics\n",
        "    accuracy = np.mean(y_pred_labels == y_true_labels)\n",
        "    weighted_precision, weighted_recall, weighted_f1, _ = precision_recall_fscore_support(\n",
        "        y_true_labels, y_pred_labels, average='weighted', zero_division=0\n",
        "    )\n",
        "    macro_precision, macro_recall, macro_f1, _ = precision_recall_fscore_support(\n",
        "        y_true_labels, y_pred_labels, average='macro', zero_division=0\n",
        "    )\n",
        "\n",
        "    print(\"📈 DETAILED PER-CLASS PERFORMANCE METRICS:\")\n",
        "    print(\"Class | Precision | Recall   | F1-Score | Support | Status\")\n",
        "    print(\"-\" * 65)\n",
        "\n",
        "    performance_targets_met = 0\n",
        "    total_classes = len(class_names)\n",
        "\n",
        "    for i, class_name in enumerate(class_names):\n",
        "        if precision[i] > 0.7 and recall[i] > 0.7:\n",
        "            status = \"✅ EXCELLENT\"\n",
        "            performance_targets_met += 1\n",
        "        elif precision[i] > 0.5 and recall[i] > 0.5:\n",
        "            status = \"📊 GOOD\"\n",
        "        elif precision[i] > 0.3 or recall[i] > 0.3:\n",
        "            status = \"⚠️ NEEDS WORK\"\n",
        "        else:\n",
        "            status = \"🔍 POOR\"\n",
        "\n",
        "        print(f\"{class_name:5} | {precision[i]:9.4f} | {recall[i]:8.4f} | {f1[i]:9.4f} | {support[i]:7d} | {status}\")\n",
        "\n",
        "    print(f\"\\n📊 OVERALL MODEL PERFORMANCE:\")\n",
        "    print(f\"Accuracy:           {accuracy:.4f}\")\n",
        "    print(f\"Weighted Precision: {weighted_precision:.4f}\")\n",
        "    print(f\"Weighted Recall:    {weighted_recall:.4f}\")\n",
        "    print(f\"Weighted F1-Score:  {weighted_f1:.4f}\")\n",
        "    print(f\"Macro Precision:    {macro_precision:.4f}\")\n",
        "    print(f\"Macro Recall:       {macro_recall:.4f}\")\n",
        "    print(f\"Macro F1-Score:     {macro_f1:.4f}\")\n",
        "\n",
        "    # Class coverage analysis\n",
        "    unique_pred = np.unique(y_pred_labels)\n",
        "    print(f\"\\n🔍 CLASS COVERAGE ANALYSIS:\")\n",
        "    print(f\"Predicted classes: {len(unique_pred)}/{len(class_names)}\")\n",
        "\n",
        "    if len(unique_pred) == len(class_names):\n",
        "        print(\"✅ All classes are being predicted\")\n",
        "    else:\n",
        "        missing = set(range(len(class_names))) - set(unique_pred)\n",
        "        print(f\"❌ Missing predictions for: {[class_names[i] for i in missing]}\")\n",
        "\n",
        "    # Performance achievement\n",
        "    print(f\"\\n🎯 PERFORMANCE SUMMARY:\")\n",
        "    if performance_targets_met == total_classes:\n",
        "        print(\"🎉 OUTSTANDING PERFORMANCE! All classes well-classified!\")\n",
        "    elif performance_targets_met >= total_classes * 0.5:\n",
        "        print(\"✅ GOOD PERFORMANCE! Most classes well-classified\")\n",
        "    else:\n",
        "        print(\"⚠️ NEEDS IMPROVEMENT! Continue training or adjust model\")\n",
        "\n",
        "    return accuracy, weighted_precision, weighted_recall, weighted_f1\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"🎯 HIGH-PERFORMANCE FEDERATED ECG ANALYSIS\")\n",
        "    print(\"⭐ Target: Balanced Performance Across All Classes\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Check if wfdb is installed\n",
        "    try:\n",
        "        import wfdb\n",
        "    except ImportError:\n",
        "        print(\"Installing required packages...\")\n",
        "        import subprocess\n",
        "        subprocess.run(['pip', 'install', 'wfdb'])\n",
        "        import wfdb\n",
        "\n",
        "    print(f\"📂 Using data path: {DATA_PATH}\")\n",
        "\n",
        "    try:\n",
        "        # Run high-performance learning\n",
        "        model, fl_system, X_test, y_test, label_encoder = run_high_performance_learning()\n",
        "\n",
        "        if model is not None:\n",
        "            # Comprehensive performance evaluation\n",
        "            accuracy, precision, recall, f1 = comprehensive_performance_evaluation(model, X_test, y_test, label_encoder)\n",
        "\n",
        "            print(f\"\\n🎉 FEDERATED LEARNING COMPLETED!\")\n",
        "            print(f\"🏆 Final Test Accuracy: {accuracy:.4f}\")\n",
        "            print(f\"🏆 Weighted Precision:  {precision:.4f}\")\n",
        "            print(f\"🏆 Weighted Recall:     {recall:.4f}\")\n",
        "            print(f\"🏆 Weighted F1-Score:   {f1:.4f}\")\n",
        "\n",
        "            # Save model\n",
        "            try:\n",
        "                model.save('high_performance_ecg_model.h5')\n",
        "                print(\"💾 Model saved: high_performance_ecg_model.h5\")\n",
        "            except Exception as e:\n",
        "                print(f\"⚠️ Could not save model: {e}\")\n",
        "\n",
        "            print(\"\\n✅ REAL DATA PROCESSING:\")\n",
        "            print(\"   • Google Drive mounting\")\n",
        "            print(\"   • Automatic path discovery\")\n",
        "            print(\"   • Multiple file pattern matching\")\n",
        "            print(\"   • Robust data loading\")\n",
        "            print(\"   • Real ECG signal processing\")\n",
        "\n",
        "        else:\n",
        "            print(\"❌ Training failed\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ ERROR: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Whm4msIdFS4l",
        "outputId": "a05e4e36-ff92-4e83-8cee-81e76d1e56fe"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🎯 HIGH-PERFORMANCE FEDERATED ECG ANALYSIS\n",
            "============================================================\n",
            "Mounted at /content/drive\n",
            "🎯 HIGH-PERFORMANCE FEDERATED ECG ANALYSIS\n",
            "⭐ Target: Balanced Performance Across All Classes\n",
            "==================================================\n",
            "📂 Using data path: /content/drive/MyDrive/mit-bih-arrhythmia-database\n",
            "\n",
            "🎯 STARTING HIGH-PERFORMANCE FEDERATED TRAINING\n",
            "==================================================\n",
            "📁 Loading Real MIT-BIH ECG Data...\n",
            "📂 Data path: /content/drive/MyDrive/mit-bih-arrhythmia-database\n",
            "📄 Found 207 files in directory\n",
            "📄 ECG files found: 96\n",
            "📄 Sample files: ['100.dat', '100.hea', '101.dat', '101.hea', '102.dat', '102.hea', '103.dat', '103.hea', '104.dat', '104.hea']\n",
            "📖 Loading record 100 from /content/drive/MyDrive/mit-bih-arrhythmia-database/100...\n",
            "   ✅ Record 100: 2271 beats\n",
            "📖 Loading record 101 from /content/drive/MyDrive/mit-bih-arrhythmia-database/101...\n",
            "   ✅ Record 101: 1862 beats\n",
            "📖 Loading record 102 from /content/drive/MyDrive/mit-bih-arrhythmia-database/102...\n",
            "   ✅ Record 102: 103 beats\n",
            "📖 Loading record 103 from /content/drive/MyDrive/mit-bih-arrhythmia-database/103...\n",
            "   ✅ Record 103: 2084 beats\n",
            "📖 Loading record 104 from /content/drive/MyDrive/mit-bih-arrhythmia-database/104...\n",
            "   ✅ Record 104: 165 beats\n",
            "📖 Loading record 105 from /content/drive/MyDrive/mit-bih-arrhythmia-database/105...\n",
            "   ✅ Record 105: 2567 beats\n",
            "📖 Loading record 106 from /content/drive/MyDrive/mit-bih-arrhythmia-database/106...\n",
            "   ✅ Record 106: 2027 beats\n",
            "📖 Loading record 107 from /content/drive/MyDrive/mit-bih-arrhythmia-database/107...\n",
            "   ✅ Record 107: 59 beats\n",
            "📖 Loading record 108 from /content/drive/MyDrive/mit-bih-arrhythmia-database/108...\n",
            "   ✅ Record 108: 1762 beats\n",
            "📖 Loading record 109 from /content/drive/MyDrive/mit-bih-arrhythmia-database/109...\n",
            "   ✅ Record 109: 2531 beats\n",
            "📖 Loading record 111 from /content/drive/MyDrive/mit-bih-arrhythmia-database/111...\n",
            "   ✅ Record 111: 2124 beats\n",
            "📖 Loading record 112 from /content/drive/MyDrive/mit-bih-arrhythmia-database/112...\n",
            "   ✅ Record 112: 2539 beats\n",
            "📖 Loading record 113 from /content/drive/MyDrive/mit-bih-arrhythmia-database/113...\n",
            "   ✅ Record 113: 1794 beats\n",
            "📖 Loading record 114 from /content/drive/MyDrive/mit-bih-arrhythmia-database/114...\n",
            "   ✅ Record 114: 1879 beats\n",
            "✅ Collected 23767 beats from 14 MIT-BIH records\n",
            "📊 Final dataset shape: (23767, 187, 1)\n",
            "📊 Label distribution: Counter({'N': 22971, 'V': 726, 'S': 62, 'F': 8})\n",
            "📊 Training samples: 17111\n",
            "📊 Validation samples: 1902\n",
            "📊 Test samples: 4754\n",
            "\n",
            "⚖️ Computing performance-optimized weights...\n",
            "📊 Class Distribution:\n",
            "   N:    5 samples (0.0%)\n",
            "   S: 16538 samples (96.7%)\n",
            "   V:   45 samples (0.3%)\n",
            "   F:  523 samples (3.1%)\n",
            "⚖️ Balanced Weights:\n",
            "   N:    5 samples | Weight: 1.00\n",
            "   S: 16538 samples | Weight: 0.50\n",
            "   V:   45 samples | Weight: 0.50\n",
            "   F:  523 samples | Weight: 0.50\n",
            "🧠 Creating high-performance model...\n",
            "🔧 Compiling model for high performance...\n",
            "\n",
            "👥 Setting up 3 clients for high performance...\n",
            "   Client 0: 4562 samples\n",
            "   Client 1: 4562 samples\n",
            "   Client 2: 4562 samples\n",
            "\n",
            "🔄 Starting High-Performance Federated Training...\n",
            "   - Gentle Differential Privacy: ✅ Enabled\n",
            "   - Balanced Loss: ✅ Enabled\n",
            "   - Class Weights: ✅ Enabled\n",
            "   - Clients: 3\n",
            "   - Rounds: 15\n",
            "   - Target: Balanced Performance\n",
            "   - Classes: 4 (F, N, S, V)\n",
            "\n",
            "🎯 Round 1/15\n",
            "   🔒 Aggregating client updates...\n",
            "   📊 Test Accuracy: 0.9666\n",
            "   🎯 Avg Precision: 0.2416, Avg Recall: 0.2500\n",
            "   💫 New best accuracy: 0.9666\n",
            "\n",
            "🎯 Round 2/15\n",
            "   🔒 Aggregating client updates...\n",
            "   📊 Test Accuracy: 0.9666\n",
            "   🎯 Avg Precision: 0.2416, Avg Recall: 0.2500\n",
            "\n",
            "🎯 Round 3/15\n",
            "   🔒 Aggregating client updates...\n",
            "   📊 Test Accuracy: 0.9750\n",
            "   🎯 Avg Precision: 0.4225, Avg Recall: 0.3624\n",
            "   💫 New best accuracy: 0.9750\n",
            "\n",
            "🎯 Round 4/15\n",
            "   🔒 Aggregating client updates...\n",
            "   📊 Test Accuracy: 0.9817\n",
            "   🎯 Avg Precision: 0.4705, Avg Recall: 0.3875\n",
            "   💫 New best accuracy: 0.9817\n",
            "\n",
            "🎯 Round 5/15\n",
            "   Client 0: Accuracy = 0.9807\n",
            "   🔒 Aggregating client updates...\n",
            "   📊 Test Accuracy: 0.9743\n",
            "   🎯 Avg Precision: 0.4672, Avg Recall: 0.3221\n",
            "\n",
            "🎯 Round 6/15\n",
            "   🔒 Aggregating client updates...\n",
            "   📊 Test Accuracy: 0.9809\n",
            "   🎯 Avg Precision: 0.4742, Avg Recall: 0.3789\n",
            "\n",
            "🎯 Round 7/15\n",
            "   🔒 Aggregating client updates...\n",
            "   📊 Test Accuracy: 0.9842\n",
            "   🎯 Avg Precision: 0.4785, Avg Recall: 0.4065\n",
            "   💫 New best accuracy: 0.9842\n",
            "\n",
            "🎯 Round 8/15\n",
            "   🔒 Aggregating client updates...\n",
            "   📊 Test Accuracy: 0.9859\n",
            "   🎯 Avg Precision: 0.4823, Avg Recall: 0.4186\n",
            "   💫 New best accuracy: 0.9859\n",
            "\n",
            "🎯 Round 9/15\n",
            "   🔒 Aggregating client updates...\n",
            "   📊 Test Accuracy: 0.9884\n",
            "   🎯 Avg Precision: 0.4774, Avg Recall: 0.4460\n",
            "   💫 New best accuracy: 0.9884\n",
            "\n",
            "🎯 Round 10/15\n",
            "   Client 0: Accuracy = 0.9860\n",
            "   🔒 Aggregating client updates...\n",
            "   📊 Test Accuracy: 0.9895\n",
            "   🎯 Avg Precision: 0.4767, Avg Recall: 0.4546\n",
            "   💫 New best accuracy: 0.9895\n",
            "\n",
            "🎯 Round 11/15\n",
            "   🔒 Aggregating client updates...\n",
            "   📊 Test Accuracy: 0.9901\n",
            "   🎯 Avg Precision: 0.4744, Avg Recall: 0.4631\n",
            "   💫 New best accuracy: 0.9901\n",
            "\n",
            "🎯 Round 12/15\n",
            "   🔒 Aggregating client updates...\n",
            "   📊 Test Accuracy: 0.9886\n",
            "   🎯 Avg Precision: 0.4634, Avg Recall: 0.4628\n",
            "\n",
            "🎯 Round 13/15\n",
            "   🔒 Aggregating client updates...\n",
            "   📊 Test Accuracy: 0.9899\n",
            "   🎯 Avg Precision: 0.4820, Avg Recall: 0.4531\n",
            "\n",
            "🎯 Round 14/15\n",
            "   🔒 Aggregating client updates...\n",
            "   📊 Test Accuracy: 0.9895\n",
            "   🎯 Avg Precision: 0.4852, Avg Recall: 0.4479\n",
            "\n",
            "🎯 Round 15/15\n",
            "   Client 0: Accuracy = 0.9901\n",
            "   🔒 Aggregating client updates...\n",
            "   📊 Test Accuracy: 0.9928\n",
            "   🎯 Avg Precision: 0.4891, Avg Recall: 0.4722\n",
            "   💫 New best accuracy: 0.9928\n",
            "\n",
            "📈 TRAINING PROGRESS ANALYSIS:\n",
            "   Round 11: Acc=0.9901, Prec=0.4744 [✅ GOOD]\n",
            "   Round 12: Acc=0.9886, Prec=0.4634 [✅ GOOD]\n",
            "   Round 13: Acc=0.9899, Prec=0.4820 [✅ GOOD]\n",
            "   Round 14: Acc=0.9895, Prec=0.4852 [✅ GOOD]\n",
            "   Round 15: Acc=0.9928, Prec=0.4891 [✅ GOOD]\n",
            "\n",
            "🏆 Best model from round 15\n",
            "   Best Accuracy: 0.9928\n",
            "\n",
            "======================================================================\n",
            "📊 COMPREHENSIVE PERFORMANCE EVALUATION\n",
            "======================================================================\n",
            "📈 DETAILED PER-CLASS PERFORMANCE METRICS:\n",
            "Class | Precision | Recall   | F1-Score | Support | Status\n",
            "-----------------------------------------------------------------\n",
            "F     |    0.0000 |   0.0000 |    0.0000 |       2 | 🔍 POOR\n",
            "N     |    0.9937 |   0.9991 |    0.9964 |    4595 | ✅ EXCELLENT\n",
            "S     |    0.0000 |   0.0000 |    0.0000 |      12 | 🔍 POOR\n",
            "V     |    0.9627 |   0.8897 |    0.9247 |     145 | ✅ EXCELLENT\n",
            "\n",
            "📊 OVERALL MODEL PERFORMANCE:\n",
            "Accuracy:           0.9928\n",
            "Weighted Precision: 0.9898\n",
            "Weighted Recall:    0.9928\n",
            "Weighted F1-Score:  0.9913\n",
            "Macro Precision:    0.4891\n",
            "Macro Recall:       0.4722\n",
            "Macro F1-Score:     0.4803\n",
            "\n",
            "🔍 CLASS COVERAGE ANALYSIS:\n",
            "Predicted classes: 2/4\n",
            "❌ Missing predictions for: ['F', 'S']\n",
            "\n",
            "🎯 PERFORMANCE SUMMARY:\n",
            "✅ GOOD PERFORMANCE! Most classes well-classified\n",
            "\n",
            "🎉 FEDERATED LEARNING COMPLETED!\n",
            "🏆 Final Test Accuracy: 0.9928\n",
            "🏆 Weighted Precision:  0.9898\n",
            "🏆 Weighted Recall:     0.9928\n",
            "🏆 Weighted F1-Score:   0.9913\n",
            "💾 Model saved: high_performance_ecg_model.h5\n",
            "\n",
            "✅ REAL DATA PROCESSING:\n",
            "   • Google Drive mounting\n",
            "   • Automatic path discovery\n",
            "   • Multiple file pattern matching\n",
            "   • Robust data loading\n",
            "   • Real ECG signal processing\n"
          ]
        }
      ]
    }
  ]
}